#--Background
Action recognition in videos has been one of the very active research fields in computer vision [1, 2] due to its wide applications in areas like surveillance, video retrieval, human-computer interaction and smart environments. In the recent decades, studies of human action recognition mainly work on video sequences captured by traditional RGB cameras. Because of individual differences, the diversity and complexity of actions, and complex backgrounds, action recognition is still a challenging problem. With the development of new RGB-D cameras, e.g. Kinect camera, capturing color images as well as depth maps has become feasible in real time. The depth maps can provide additional cues, such as body shape and motion information to discriminate actions. Due to these advantages, recent research trend concentrates on exploiting depth maps for action recognition [3–12].

#--Motivation
For action recognition, motion is crucial information to represent actions in videos. In order to capture motion information, there are two major approaches, including: detecting salient point-based approach, and tracking point-based approach. The first approach, as described in works [13–18], generates a compact video representation and accepts background clutter, occlusions and scale changes. However, the approach only detects points with dramatic changes in both spatio-temporal directions, it does not capture moving patterns appear in videos. Therefore, this approach may deal with several challenges to recognize complicated motions. An other approach is based on tracking sampled points in videos along time dimension. The approach does not only detect salient points, but also capture moving patterns This approach has demonstrated the effectiveness in motionbased problems, such as: action recognition, multimedia event detection (MED), video mining, so on. Although, it is one of the state-of-the-art approaches to capture motion information exactly, in our best knowledge, there is no work extend to depth video. In this paper, we considers to exploit the trajectory-based approach on depth video.

#--Proposed method
In order to adapt the trajectory-based approach on depth video, we must transform depth video to corresponding intensity video. This issue can be easily performed by considering depth value as intensity value. However, this way can cause confused cases of action classes. For example, forward punch and hammer may be confused actions, if we view them from front, since they contain similar movements respectively: “lift arm up” and “stretch out”. Therefore the additional information is needed to distinguish such actions.

The basis idea to deal with such cases is to watch actions from various views. Information achieved from the views can provide clearer cues to discriminate such actions. To collect such information from depth video, a simple way is to project depth maps onto view planes. The projections are easily obtained by the mentioned advantages of depth data. Data projected on the planes is then gathered to generate corresponding intensity videos. Trajectory-based motion features are then calculated on intensity videos to generate a final feature
representation for depth video. The final feature representation can be regarded as a representation of trajectories in 3D space, called Pseudo-3D Trajectories.

#--Experiment
To evaluate the effectiveness of our method, we conduct experiments on MSR Action 3D dataset. Experimental results show that our proposed method beats the state-of-the-art methods in constrain of only using depth data. The results also present our contributions: (1) we propose an effective method to exploit trajectories in depth video, (2) we perform comprehensive experiments on the challenging benchmark dataset and indicate that our proposed method is the best when compared with the state-of-the-art depth-based methods.

#--Paper structure
After a brief review of the related work in Section 2, the proposed method is described in Section 3. Sections 4 and 5 present the experimental settings and results. In section 6 we provide some concerned discussions. The summaries of our work are given in Section 7.