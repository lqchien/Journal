#--Background
Action recognition in videos has been one of the very active research fields in computer vision [7, 14] due to its wide applications in areas like surveillance, video retrieval, human-computer interaction and smart environments. 
In the recent decades, studies of human action recognition mainly work on video sequences captured by traditional RGB cameras. 
Because of individual differences, the diversity and complexity of actions, and complex backgrounds, action recognition is still a challenging problem. 
With the development of new RGB-D cameras, e.g. Kinect camera, capturing color images as well as depth maps has become feasible in real time.
The depth maps can provide additional cues, such as body shape and motion information to discriminate actions.
In addition, the new data type is able to generate similar projections from a single view.
Due to these advantages, recent research trend concentrates on exploiting depth maps for action recognition [1-4,...].

#--Motivation
For action recognition, motion is crucial information to represent actions in videos.
In order to capture motion information, there are two major approaches, including: space-time interest point (STIP)-based approach, and trajectory-based approach.
The STIP-based approach generates a compact video representation and accepts background clutter, occlusions and scale changes [x,x,x,x].
However, the approach only detects points with dramatic changes in both spatio-temporal directions, but not provide moving patterns appear in videos.
To deal with this issue, the trajectory-based approach is mainly based on tracking sampled points in videos along the time dimension.
Although, this approach is one of the state-of-the-art solutions [y,y,y,y] to capture motion information exactly, in our best knowledge, there is no work extend to depth video.
In this paper, we considers to exploit the trajectory-based approach to depth video.

#--Proposed method
In order to apply the trajectory-based approach on depth video, firstly depth video must be transformed to corresponding intensity video. This issue can be easily performed by considering depth value as intensity value. However, this way can cause confused cases of action classes. For example, \textit{forward punch} and \textit{hammer} may be confused actions, if we view them from front, since they contain similar movements respectively: ``lift arm up'' and ``stretch out''. Therefore the additional information is needed to distinguish such actions.

The basis idea to deal with such cases is to watch actions from various views. Information achieved from the views can provide clearer cues to discriminate such actions. To collect such information from depth video, a simple way is to project depth maps onto view planes. The projections are easily obtained by the mentioned advantages of depth data. Data projected on the planes is then gathered to generate corresponding intensity videos. Trajectory-based motion features are, then, calculted on each intensity video to generate a final feature representation for depth video. The final feature representation can be regarded as a representation of trajectories in 3D space, called \textit{Pseudo-3D Trajectories}.


#--Experiment
To evaluate the effectiveness of our method, we conduct experiments on MSR Action 3D dataset and MSR Daily Activity 3D dataset. Experimental results show that our proposed method beats the state-of-the-art methods in constrain of only using depth data. The results also present our contributions: (1) we propose an adaptive method for depth video representation by using intensity-based features, (2) we perform comprehensive experiments on the challenging benchmark datasets and indicate that our method is the best when compared with the state-of-the-art depth-based methods.

#--Structure of paper
After a brief review of the related work in Section \ref{lbl:RelatedWorks}, the proposed method is described in Section \ref{lbl:ProposedMethod}. Sections \ref{lbl:ExperimentalSettings} and \ref{lbl:ExperimentalResults} present the experimental settings and results. In section \ref{lbl:Discussions} we provide some concerned discussions. The summaries of our work are given in Section \ref{lbl:Conclusions}.