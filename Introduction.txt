#--Background
Action recognition in videos has been one of the very active research fields in computer vision \cite{pirsiavash2012detecting, poppe2010survey} due to its wide applications in areas like surveillance, video retrieval, human-computer interaction and smart environments.
In the recent decades, studies of human action recognition mainly work on video sequences captured by traditional RGB cameras.
Because of individual differences, the diversity and complexity of actions, and complex backgrounds, action recognition is still a challenging problem.
In domain of 2D videos, two approaches are used popularly, including: spatio-temporal interest points (STIP)-based approach and trajectory-based approach.
The first approach, as described in works \cite{laptev2005space, dollar2005behavior, laptev2008learning, bregonzio2009recognising, klaser2008aspatiotemporal, willems2008efficient}, generates a compact video representation and accepts background clutter, occlusions and scale changes.
However, in case of recognizing complicated motions, the approach deals with several challenges, due to the lack of relationship of interest points.
In recent studies \cite{matikainen2009trajectons, messing2009activity, sun2009hierarchical}, an other approach based on trajectories captures moving patterns in video, thereby it provides additional information to recognize motions more exactly.
H.Wang et al. [DenseTraj] has demonstrated that dense trajectory-based approach is the state-of-the-art approach for action recognition.

#--Motivation
With the development of new RGB-D cameras, e.g. Kinect camera, capturing color images as well as depth maps has become feasible in real time.
The depth maps can provide additional cues, such as body shape and motion information to discriminate actions.
Due to these advantages, recent research trend concentrates on exploiting depth maps for action recognition [3–12].
In our best knowledge, none success with combining dense trajectories, the state-of-the-art approach on 2D video, and depth video.
In this paper, we investigate to exploit the dense trajectory-based approach on depth video.

#--Proposed method
In order to exploit the 2D approach on depth video, we must transform depth video to corresponding 2D video.
This issue can be easily performed by considering depth value as intensity value.
However, this way can cause confused cases of action classes.
For example, forward punch and hammer may be confused actions, if we view them from front, since they contain similar movements respectively: “lift arm up” and “stretch out”.
Therefore the additional information is needed to distinguish such actions.

The basis idea to deal with such cases is to watch actions from various views.
Information achieved from the views can provide clearer cues to discriminate such actions.
To collect such information from depth video, a simple way is to project depth maps onto view planes.
The projections are easily obtained by the mentioned advantages of depth data.
Data projected on the planes is then gathered to generate corresponding 2D videos.
Dense trajectory-based motion features are then calculated on 2D videos to generate a final feature representation for depth video.

#--Experiment
To evaluate the effectiveness of our method, we conduct experiments on MSR Action 3D dataset and MSR Daily Activity 3D dataset.
Experimental results show that our proposed method beats the state-of-the-art methods in constrain of only using depth data.
The results also present our contributions: (1) we propose an effective method to exploit trajectories in depth video, (2) we perform comprehensive experiments on the challenging benchmark dataset and indicate that our proposed method is the best when compared with the state-of-the-art depth-based methods.

#--Paper structure
After a brief review of the related work in Section 2, the proposed method is described in Section 3. Sections 4 and 5 present the experimental settings and results. In section 6 we provide some concerned discussions. The summaries of our work are given in Section 7.